{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8241e",
   "metadata": {},
   "source": [
    "# Similarity-based Competency Extraction Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94fcf2",
   "metadata": {},
   "source": [
    "* First we check if the required packages are installed. \n",
    "\n",
    "spaCy: an open-source software library for advanced natural language processing, written in the programming languages Python and Cython \n",
    "\n",
    "TensorFlow: a free and open-source software library for machine learning and artificial intelligence.\n",
    "\n",
    "NumPy: a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "\n",
    "pandas: a software library written for the Python programming language for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51ef929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install pandas\n",
    "!pip3 install -U pip setuptools wheel\n",
    "!pip3 install -U spacy\n",
    "!python3 -m spacy download de_core_news_lg\n",
    "!pip3 install absl-py\n",
    "!pip3 install --upgrade tensorflow\n",
    "!pip3 install \"tensorflow>=2.0.0\"\n",
    "!pip3 install --upgrade tensorflow-hub\n",
    "!pip3 install tensorflow_text\n",
    "!pip3 install --ignore-installed Pillow==9.0.0\n",
    "!pip3 install xmltodict\n",
    "!pip3 install markupsafe==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449110e0",
   "metadata": {},
   "source": [
    "* Now we can import these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a644c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re  # RegEx\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "import xmltodict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5753ea0",
   "metadata": {},
   "source": [
    "* We load the tensorflow module for text embedding.  \n",
    "\n",
    "The main idea of the similarity calculation part is to vectorize the text and then calculate the cosine distance between them.\n",
    "Machine learning models take vectors as input and the word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding.  \n",
    "For this, we use a pre-trained transformer model on TensorFlow hub.\n",
    "This specific module is optimized for multi-word length text, such as sentences, phrases, or short paragraphs.\n",
    "And it converts variable-length text into a 512 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c792cc3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 22:06:49.313037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3 loaded\n",
      "CPU times: user 6.3 s, sys: 458 ms, total: 6.76 s\n",
      "Wall time: 7.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be90ea4",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d03d3",
   "metadata": {},
   "source": [
    "We use the ESCO skills dataset as our corpus. You can download the latest version of it here https://esco.ec.europa.eu/en/use-esco/download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b9065",
   "metadata": {},
   "source": [
    "* import competency data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e2e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.2 ms, sys: 21 ms, total: 85.3 ms\n",
      "Wall time: 89.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skills = pd.read_csv('data/skills_de.csv',usecols=['conceptUri','preferredLabel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad027a7",
   "metadata": {},
   "source": [
    "Note that for efficiency reasons we only use preferred term (contents of column \"preferredLabel\") for our extraction service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43edda",
   "metadata": {},
   "source": [
    "### further: previous label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7880c532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conceptUri</th>\n",
       "      <th>preferredLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://data.europa.eu/esco/skill/0005c151-5b5a...</td>\n",
       "      <td>Musikpersonal verwalten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://data.europa.eu/esco/skill/00064735-8fad...</td>\n",
       "      <td>Strafvollzugsverfahren beaufsichtigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://data.europa.eu/esco/skill/000709ed-2be5...</td>\n",
       "      <td>nicht unterdrückende Praktiken anwenden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://data.europa.eu/esco/skill/0007bdc2-dd15...</td>\n",
       "      <td>Einhaltung von Vorschriften von Eisenbahnfahrz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://data.europa.eu/esco/skill/00090cc1-1f27...</td>\n",
       "      <td>verfügbare Dienste ermitteln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://data.europa.eu/esco/skill/000bb1e4-89f0...</td>\n",
       "      <td>toxikologische Studien durchführen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://data.europa.eu/esco/skill/000c94d2-2a2e...</td>\n",
       "      <td>Einheitlichkeit von Kokillen gewährleisten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://data.europa.eu/esco/skill/000f1d3d-220f...</td>\n",
       "      <td>Haskell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://data.europa.eu/esco/skill/001115fb-569f...</td>\n",
       "      <td>Initiative zeigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://data.europa.eu/esco/skill/001d46db-035e...</td>\n",
       "      <td>Personal mit Blick auf die Verringerung von Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conceptUri  \\\n",
       "0  http://data.europa.eu/esco/skill/0005c151-5b5a...   \n",
       "1  http://data.europa.eu/esco/skill/00064735-8fad...   \n",
       "2  http://data.europa.eu/esco/skill/000709ed-2be5...   \n",
       "3  http://data.europa.eu/esco/skill/0007bdc2-dd15...   \n",
       "4  http://data.europa.eu/esco/skill/00090cc1-1f27...   \n",
       "5  http://data.europa.eu/esco/skill/000bb1e4-89f0...   \n",
       "6  http://data.europa.eu/esco/skill/000c94d2-2a2e...   \n",
       "7  http://data.europa.eu/esco/skill/000f1d3d-220f...   \n",
       "8  http://data.europa.eu/esco/skill/001115fb-569f...   \n",
       "9  http://data.europa.eu/esco/skill/001d46db-035e...   \n",
       "\n",
       "                                      preferredLabel  \n",
       "0                            Musikpersonal verwalten  \n",
       "1              Strafvollzugsverfahren beaufsichtigen  \n",
       "2            nicht unterdrückende Praktiken anwenden  \n",
       "3  Einhaltung von Vorschriften von Eisenbahnfahrz...  \n",
       "4                       verfügbare Dienste ermitteln  \n",
       "5                 toxikologische Studien durchführen  \n",
       "6         Einheitlichkeit von Kokillen gewährleisten  \n",
       "7                                            Haskell  \n",
       "8                                  Initiative zeigen  \n",
       "9  Personal mit Blick auf die Verringerung von Le...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c615675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  Musikpersonal verwalten\n",
       "1                    Strafvollzugsverfahren beaufsichtigen\n",
       "2                  nicht unterdrückende Praktiken anwenden\n",
       "3        Einhaltung von Vorschriften von Eisenbahnfahrz...\n",
       "4                             verfügbare Dienste ermitteln\n",
       "                               ...                        \n",
       "13886    berufliche Leistungsfähigkeit von Nutzern/Nutz...\n",
       "13887             Beleuchtung in Transportgeräten einbauen\n",
       "13888                     Verarbeitung natürlicher Sprache\n",
       "13889                             Bauarbeiten koordinieren\n",
       "13890         Absturzsicherungen und Bordbretter anbringen\n",
       "Name: preferredLabel, Length: 13891, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = skills['preferredLabel'].copy(deep=True)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65084a27",
   "metadata": {},
   "source": [
    "We use spaCy processing pipeline to do the lemmatization of each token. After lemmatization, all punctuations will  be converted to \"--\" and then deleted.\n",
    "\n",
    "Since the data were obtained from the official documents of the European Commission, we did not perform data cleaning on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b954f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_processing(text):\n",
    "    processed = []\n",
    "    for item in text:\n",
    "        item = nlp(item)\n",
    "        itemString = ''\n",
    "        for word in item:\n",
    "            word = word.lemma_.lower() # lemma_: Extract the lemma for each token\n",
    "            if word != '--' and word != '' and word != ' ':\n",
    "#                 if word == '\\xa0': continue                    \n",
    "                itemString += word + ' '\n",
    "        processed.append(itemString[:-1])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c626ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 1.15 s, total: 1min 12s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processedLabels = labels_processing(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085bfe1",
   "metadata": {},
   "source": [
    "* import descriptions data from xml file\n",
    "\n",
    "Here you can try this course_description_testset.xml file to get the result of test dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319c33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = open('data/course_description_testset.xml','r', encoding='ISO-8859-15').read() # read data\n",
    "test_dict = xmltodict.parse(test) # parse xml\n",
    "xmlDict = test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cdc66",
   "metadata": {},
   "source": [
    "Load all course description data from xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec9edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.95 s, sys: 506 ms, total: 9.46 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# xml_data = open('data/course_description_FOKUS.xml','r', encoding='ISO-8859-15').read() # read data\n",
    "# xmlDict = xmltodict.parse(xml_data) # parse xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c236bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e24004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xmlcourses = xmlDict['DEFTISCAT']['COURSETRANSACTIONS']['INSERTCOURSES']['COURSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e02899",
   "metadata": {},
   "source": [
    "Competency is often included in course titles. So we combine course_name and course_description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3209b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 ms, sys: 3.66 ms, total: 40.3 ms\n",
      "Wall time: 76.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coursesDict = []\n",
    "for course in xmlcourses:\n",
    "    coursesDict.append({\"course_id\": course['CS_ID'], \n",
    "                        \"course_name\": course['CS_NAME'],\n",
    "                 \"course_description\": course['CS_DESC_LONG']})\n",
    "courses_table = pd.DataFrame(coursesDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86582e8",
   "metadata": {},
   "source": [
    "Before combining the text, we need to deal with the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999d3356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "course_id              0\n",
       "course_name            0\n",
       "course_description    97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_null = courses_table.isnull().sum(axis=0)\n",
    "col_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82117601",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_table.replace([None],value='',inplace=True)\n",
    "courses = courses_table['course_name'] + ' ' + courses_table['course_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c8b518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4A873264-7ADD-DE47-3039-1FDA692E8164</td>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>- Analyse strapaziöser Gesprächsmuster\\n- Schw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99BEDCEB-4FF3-3F8F-88B7-B30E50638F01</td>\n",
       "      <td>Aktuelles Arbeitsrecht 2022</td>\n",
       "      <td>Kurzbeschreibung\\nDas Arbeitsrecht unterliegt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97FD41FD-1A91-179C-3EA5-6A23E2F436D5</td>\n",
       "      <td>Ambulante Pflege - Rechtssicher Handeln und Ha...</td>\n",
       "      <td>- Grundlagen der straf- und zivilrechtlichen H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0CC573A-79D7-79A4-5B22-F675C4F06950</td>\n",
       "      <td>Aufgaben des gesetzlichen Betreuers - Zur Refo...</td>\n",
       "      <td>Kurzbeschreibung\\nNoch im Jahr 2020 plant der ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7C449EF7-12A8-82FC-0E5E-BD2885FDB93C</td>\n",
       "      <td>Basisqualifikation für ungelernte Pflegekräfte...</td>\n",
       "      <td>- Alten- und Krankenpflege\\n  . Körperpflege\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16847</th>\n",
       "      <td>A1E046BE-1C4B-B977-C9B9-88D8D2860107</td>\n",
       "      <td>5 Monate Weiterbildung: Organisation &amp; Führung...</td>\n",
       "      <td>Die aktuell vorherrschende Situation auf dem A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16848</th>\n",
       "      <td>A1DF8EEB-D317-70BA-508F-AFC732369860</td>\n",
       "      <td>Conversion und Usability Experte</td>\n",
       "      <td>Ziel der Maßnahme ist es den Teilnehmern eine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16849</th>\n",
       "      <td>A1DD9D98-2BA3-3A11-0A24-72FC4C79422A</td>\n",
       "      <td>Digital Transformation Management</td>\n",
       "      <td>Ziel der Maßnahme ist es den Teilnehmern eine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16850</th>\n",
       "      <td>A1DD60F7-9B6E-ECEE-F1DF-53FED2DCCC5C</td>\n",
       "      <td>E-Commerce Geschäftsmodelle</td>\n",
       "      <td>Ziel der Maßnahme ist es den Teilnehmern eine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16851</th>\n",
       "      <td>A1E01A90-D165-5959-2102-771B5459852B</td>\n",
       "      <td>Experte im Digital Content Creation</td>\n",
       "      <td>Die Teilnehmer werden zu einem Digital Content...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16852 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  course_id  \\\n",
       "0      4A873264-7ADD-DE47-3039-1FDA692E8164   \n",
       "1      99BEDCEB-4FF3-3F8F-88B7-B30E50638F01   \n",
       "2      97FD41FD-1A91-179C-3EA5-6A23E2F436D5   \n",
       "3      A0CC573A-79D7-79A4-5B22-F675C4F06950   \n",
       "4      7C449EF7-12A8-82FC-0E5E-BD2885FDB93C   \n",
       "...                                     ...   \n",
       "16847  A1E046BE-1C4B-B977-C9B9-88D8D2860107   \n",
       "16848  A1DF8EEB-D317-70BA-508F-AFC732369860   \n",
       "16849  A1DD9D98-2BA3-3A11-0A24-72FC4C79422A   \n",
       "16850  A1DD60F7-9B6E-ECEE-F1DF-53FED2DCCC5C   \n",
       "16851  A1E01A90-D165-5959-2102-771B5459852B   \n",
       "\n",
       "                                             course_name  \\\n",
       "0      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "1                            Aktuelles Arbeitsrecht 2022   \n",
       "2      Ambulante Pflege - Rechtssicher Handeln und Ha...   \n",
       "3      Aufgaben des gesetzlichen Betreuers - Zur Refo...   \n",
       "4      Basisqualifikation für ungelernte Pflegekräfte...   \n",
       "...                                                  ...   \n",
       "16847  5 Monate Weiterbildung: Organisation & Führung...   \n",
       "16848                   Conversion und Usability Experte   \n",
       "16849                  Digital Transformation Management   \n",
       "16850                        E-Commerce Geschäftsmodelle   \n",
       "16851                Experte im Digital Content Creation   \n",
       "\n",
       "                                      course_description  \n",
       "0      - Analyse strapaziöser Gesprächsmuster\\n- Schw...  \n",
       "1      Kurzbeschreibung\\nDas Arbeitsrecht unterliegt ...  \n",
       "2      - Grundlagen der straf- und zivilrechtlichen H...  \n",
       "3      Kurzbeschreibung\\nNoch im Jahr 2020 plant der ...  \n",
       "4      - Alten- und Krankenpflege\\n  . Körperpflege\\n...  \n",
       "...                                                  ...  \n",
       "16847  Die aktuell vorherrschende Situation auf dem A...  \n",
       "16848  Ziel der Maßnahme ist es den Teilnehmern eine ...  \n",
       "16849  Ziel der Maßnahme ist es den Teilnehmern eine ...  \n",
       "16850  Ziel der Maßnahme ist es den Teilnehmern eine ...  \n",
       "16851  Die Teilnehmer werden zu einem Digital Content...  \n",
       "\n",
       "[16852 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9451bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \"Schwierige\" Klienten? - Mit Patienten, Angehö...\n",
       "1        Aktuelles Arbeitsrecht 2022 Kurzbeschreibung\\n...\n",
       "2        Ambulante Pflege - Rechtssicher Handeln und Ha...\n",
       "3        Aufgaben des gesetzlichen Betreuers - Zur Refo...\n",
       "4        Basisqualifikation für ungelernte Pflegekräfte...\n",
       "                               ...                        \n",
       "16847    5 Monate Weiterbildung: Organisation & Führung...\n",
       "16848    Conversion und Usability Experte Ziel der Maßn...\n",
       "16849    Digital Transformation Management Ziel der Maß...\n",
       "16850    E-Commerce Geschäftsmodelle Ziel der Maßnahme ...\n",
       "16851    Experte im Digital Content Creation Die Teilne...\n",
       "Length: 16852, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a4483",
   "metadata": {},
   "source": [
    "We found some parts of the text that were not encoded correctly. Let's correct them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23cdbc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 541 ms, sys: 6.35 ms, total: 548 ms\n",
      "Wall time: 591 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "length = len(courses)\n",
    "for i in range(length):\n",
    "    courses[i] = courses[i].replace('&#8211;','-')\n",
    "    courses[i] = courses[i].replace('&#8222;','„')\n",
    "    courses[i] = courses[i].replace('&#8220;','“')\n",
    "    courses[i] = courses[i].replace('&#8230;','...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bf021",
   "metadata": {},
   "source": [
    "Now we split each description in sentences.\n",
    "We put the data into the spacy pipeline, remove the special symbols.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bc88773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def To_sentences(courses):\n",
    "    courses_sent = []\n",
    "    courses_sent_lemma = []\n",
    "    for course in (courses):\n",
    "        with nlp.select_pipes(disable=['attribute_ruler', 'ner']):\n",
    "            doc = nlp(str(course))\n",
    "            sentences = []\n",
    "            for sent in doc.sents:\n",
    "                course_processed = ''\n",
    "                sents = []\n",
    "                for word in sent:\n",
    "                    word = word.text\n",
    "                    word = word.replace('\\n', ' ')\n",
    "                    word = word.strip().strip('\"@#$%^&*§')\n",
    "                    if word == '-':\n",
    "                        if course_processed != '':\n",
    "                            sents.append(course_processed[:-1])\n",
    "                            course_processed = ''\n",
    "                        continue\n",
    "                    if word.startswith('- ') and len(word) > 1:\n",
    "                        word = word.replace('- ', '')\n",
    "                        if course_processed != '':\n",
    "                            sents.append(course_processed[:-1])\n",
    "                            course_processed = ''\n",
    "                    if (not re.match('\\s+', word,re.I)) and word != '--' and word != '':\n",
    "                        if \"/-\" in word: \n",
    "                            word = word.split(\"/-\")[0]\n",
    "                        if re.match(r'\\,|\\.|\\?|\\!|\\)', word):\n",
    "                            course_processed = course_processed[:-1] + word + ' '\n",
    "                        elif re.match(r'\\(',word):\n",
    "                            course_processed += word\n",
    "                        else:\n",
    "                            course_processed += word + ' '\n",
    "                sentences += sents\n",
    "                if course_processed != '':\n",
    "                    sentences.append(course_processed[:-1])\n",
    "            courses_sent_lemma.append(sentences)\n",
    "    return courses_sent_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dfef5",
   "metadata": {},
   "source": [
    "You can try and see the processing result for the first course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "856cce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Schwierige\" Klienten? - Mit Patienten, Angehörigen und Kollegen clever kommunizieren - Analyse strapaziöser Gesprächsmuster\n",
      "- Schwierige Klienten oder schwierige Situation?\n",
      "- Was heißt es schwierige Gespräche souverän zu meistern?\n",
      "- Übungen mit Videofeedback werden durchgeführt!\n",
      "[['Schwierige Klienten?', 'Mit Patienten, Angehörigen und Kollegen clever kommunizieren', 'Analyse strapaziöser Gesprächsmuster', 'Schwierige Klienten oder schwierige Situation?', 'Was heißt es schwierige Gespräche souverän zu meistern?', 'Übungen mit Videofeedback werden durchgeführt!']]\n"
     ]
    }
   ],
   "source": [
    "course_in_sentences_test = To_sentences(courses.iloc[:1])\n",
    "print(courses.iloc[0])\n",
    "print(course_in_sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b647090",
   "metadata": {},
   "source": [
    "To increase the precision of TF working result, course description is split into bunch of chunks with key words of competencies.\n",
    "For more information see https://spacy.io/api/pipeline-functions#merge_noun_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8615e4",
   "metadata": {},
   "source": [
    "First we add 'merge_noun_chunks' to the spacy processing pipeline so that noun chunks will then be seen as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "005c9559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a652c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'morphologizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627335f",
   "metadata": {},
   "source": [
    "Then we extract the Noun Phrase Chunks from the sentence.  \n",
    "Chunking identifies groups of words that go together to form symbolic meaning.  \n",
    "It builds upon the process Part of Speech (POS) Tagging, which is a way to describe the grammatical function of a word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c44ec",
   "metadata": {},
   "source": [
    "Chunking is a critical step in our approach. Because it allows us to split the course description into a collection of phrases to compare with the competencies.\n",
    "Finally, we remove the stop words, punctuation and other special symbols such as dash and slash. Then we lemmatize the remaining words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ff0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Segment_processing(part_of_the_courses):\n",
    "    \n",
    "    # to sentences\n",
    "    course_in_sentences = To_sentences(part_of_the_courses)\n",
    "    \n",
    "    # to chunks\n",
    "    courses_chunks = []\n",
    "    for course in course_in_sentences:\n",
    "        course_chunks = []\n",
    "        for sent in course:\n",
    "            sent_chunks = []\n",
    "            doc = nlp(sent)\n",
    "            for tok in doc:\n",
    "                with nlp.select_pipes(disable=['merge_noun_chunks','attribute_ruler', 'ner']):\n",
    "                    if not re.match('\\(|\\)|\\,|\\.|\\!|\\?|\\/',tok.text) and not tok.is_stop: \n",
    "                        tok = nlp(tok.text)\n",
    "                        words = ''\n",
    "                        for word in tok:\n",
    "                            if not word.is_stop and word.lemma_ != '--':\n",
    "                                words = words + word.lemma_ + ' '  \n",
    "                        sent_chunks.append(words[:-1])\n",
    "            course_chunks += sent_chunks\n",
    "        courses_chunks.append(course_chunks)\n",
    "    return courses_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6641a",
   "metadata": {},
   "source": [
    "For example the processing result of the first course is as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b6ef4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schwierige Klienten?', 'Mit Patienten, Angehörigen und Kollegen clever kommunizieren', 'Analyse strapaziöser Gesprächsmuster', 'Schwierige Klienten oder schwierige Situation?', 'Was heißt es schwierige Gespräche souverän zu meistern?', 'Übungen mit Videofeedback werden durchgeführt!']]\n",
      "[['schwierige Klient', 'Patient', 'angehörige', 'Kollege', 'clever', 'kommunizieren', 'Analyse', 'strapaziös Gesprächsmuster', 'schwierig Klient', 'schwierig Situation', 'schwierig Gespräch', 'souverän', 'Meister', 'Übung', 'videofeedback', 'durchführen']]\n",
      "CPU times: user 637 ms, sys: 846 ms, total: 1.48 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "courses_chunks_test = Segment_processing(courses.iloc[:1])\n",
    "print(course_in_sentences_test)\n",
    "print(courses_chunks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223e4f9",
   "metadata": {},
   "source": [
    "## Embedding-phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33761b0",
   "metadata": {},
   "source": [
    "After the text has been processed, we now embed them as vectors.  \n",
    "For efficiency, we will handle course descriptions in batches. (cf. function Save_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa9928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Course_Embedding(courses_chunks):\n",
    "    courses_chunks_embed = []\n",
    "    for course in courses_chunks:\n",
    "        courses_chunks_embed.append(embed(course))\n",
    "    return courses_chunks_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "027e378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 33.5 s, total: 2min 14s\n",
      "Wall time: 59.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels_embed = embed(processedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1b577",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192145",
   "metadata": {},
   "source": [
    "We calculate the cosine distance for the transformed text of competency and word chunks of course descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f78c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Similarity(labels, courses):\n",
    "    result = []\n",
    "    for course in courses:\n",
    "        result.append(np.dot(labels, np.transpose(course)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aa0fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Get_relations(relations,skills,courses_table,course_idx,benchmark,courses_chunks):\n",
    "\n",
    "    idx = 0\n",
    "    counter = 0\n",
    "\n",
    "    df_relations = pd.DataFrame(columns = ['conceptUri'])\n",
    "    for relationship in relations:\n",
    "        print('course: ', course_idx)\n",
    "        competency_idx = np.where(relationship>benchmark) \n",
    "\n",
    "        print('numbers of extracted competency: ', len(competency_idx[0]))\n",
    "        for j in range(len(competency_idx[0])):\n",
    "            print(processedLabels[competency_idx[0][j]],'--', courses_chunks[idx][competency_idx[1][j]])\n",
    "            \n",
    "        competency = set(skills.loc[list(competency_idx[0]),'conceptUri'])\n",
    "        competency_uri = pd.DataFrame(competency,columns=['conceptUri'], index=[courses_table.iloc[course_idx]['course_id']]*len(competency)) \n",
    "\n",
    "        print(' ------')\n",
    "        print('\\n')\n",
    "\n",
    "        df_relations = pd.concat([df_relations,competency_uri])\n",
    "    \n",
    "        course_idx += 1 \n",
    "        \n",
    "#         test:\n",
    "        idx += 1\n",
    "        counter += len(competency)\n",
    "    print('number of extracted competency', counter)\n",
    "    print('\\n', df_relations)\n",
    "\n",
    "    return df_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1f908",
   "metadata": {},
   "source": [
    "The performance of this method is strongly related to the setting of the threshold value.\n",
    "Theoretically, after Setting different thresholds to determine whether the texts are similar, comparing the results, and finding the threshold that makes the highest F-score.\n",
    "We can use this f-score to evaluate our method.\n",
    "However, as we said in the section on the evaluation method,\n",
    "We do not have a reliable test dataset and thus no f-score.\n",
    "We have tried several thresholds and made a manual selection based on the output results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0684f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Competence_extraction(courses,course_start_idx,labels,benchmark):\n",
    "    courses_chunks = Segment_processing(courses)\n",
    "    courses_embed = Course_Embedding(courses_chunks)\n",
    "    calculated_results = Similarity(labels, courses_embed)\n",
    "    return Get_relations(calculated_results,skills,courses_table,course_start_idx,benchmark,courses_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2825a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Save_results(courses,benchmark):\n",
    "    length = len(courses)\n",
    "    n = length//500\n",
    "    for i in range(n+1):\n",
    "        start = i * 500\n",
    "        if i == n:\n",
    "          end = length\n",
    "        else:\n",
    "          end = (i+1) * 500\n",
    "        courses_div = courses.iloc[start:end]\n",
    "        relations = Competence_extraction(courses_div,start,labels_embed,benchmark)\n",
    "        relations.to_csv(path_or_buf=\"/output/relations_part_\"+str(i)+\".csv\" ,columns=['conceptUri'], index_label='course_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af7ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Save_results(courses,benchmark=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28484f19",
   "metadata": {},
   "source": [
    "merge csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(courses)\n",
    "n = length//500\n",
    "df_relations_merge = pd.DataFrame()\n",
    "for i in range(n+1): \n",
    "    df = pd.read_csv('/output/relations_part_' + str(i) + '.csv',index_col=\"course_id\")\n",
    "    df_relations_merge = pd.concat([df_relations_merge,df])\n",
    "df_relations_merge.to_csv(path_or_buf=\"/output/relations.csv\",columns=['conceptUri'], index_label='course_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awt2022",
   "language": "python",
   "name": "awt2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
